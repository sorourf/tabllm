{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabLLM Replication: Few-shot Classification of Tabular Data\n",
    "\n",
    "This notebook replicates the TabLLM approach from Hegselmann et al., 2023 (https://github.com/clinicalml/TabLLM)\n",
    "\n",
    "**Key Components:**\n",
    "- Text Template Serialization: Convert rows to natural language (\"The [column] is [value]\")\n",
    "- Model: T0-3B with PEFT/LoRA fine-tuning\n",
    "- Training: Full training data (optimized for Google Colab free GPU)\n",
    "- Evaluation: Comprehensive metrics (Log Loss, AUC, Accuracy, Precision, Recall, F1)\n",
    "\n",
    "**Datasets:**\n",
    "1. Postpartum Depression\n",
    "2. Student Depression\n",
    "3. AI Tool Usage by Indian College Students\n",
    "4. Hilton Employee Retention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n# Note: Using bitsandbytes 0.43.0+ for CUDA 12.6 support\n!pip install -q transformers==4.36.2 datasets==2.16.1 peft==0.7.1 accelerate==0.25.0 bitsandbytes>=0.43.0 scikit-learn==1.3.2 pandas numpy tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from sklearn.metrics import (\n",
    "    log_loss, roc_auc_score, accuracy_score, \n",
    "    precision_recall_fscore_support, classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Template Serialization Function\n",
    "\n",
    "Based on TabLLM's approach, we convert each row into natural language sentences.\n",
    "Format: \"The [column name] is [value].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_row_to_text(row, exclude_cols=None, feature_descriptions=None):\n",
    "    \"\"\"\n",
    "    Convert a dataframe row to natural language text using TabLLM's template approach.\n",
    "    \n",
    "    Args:\n",
    "        row: pandas Series representing a single row\n",
    "        exclude_cols: list of column names to exclude (e.g., target, id columns)\n",
    "        feature_descriptions: dict mapping column names to human-readable descriptions\n",
    "    \n",
    "    Returns:\n",
    "        str: Natural language representation of the row\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    \n",
    "    sentences = []\n",
    "    for col, value in row.items():\n",
    "        # Skip excluded columns and NaN values\n",
    "        if col in exclude_cols or pd.isna(value):\n",
    "            continue\n",
    "        \n",
    "        # Use feature description if available, otherwise use column name\n",
    "        if feature_descriptions and col in feature_descriptions:\n",
    "            col_name = feature_descriptions[col]\n",
    "        else:\n",
    "            # Clean column name: replace underscores with spaces\n",
    "            col_name = col.replace('_', ' ').lower()\n",
    "        \n",
    "        # Format the sentence\n",
    "        sentences.append(f\"The {col_name} is {value}.\")\n",
    "    \n",
    "    return \" \".join(sentences)\n",
    "\n",
    "\n",
    "def create_classification_prompt(text, question, choices=[\"No\", \"Yes\"]):\n",
    "    \"\"\"\n",
    "    Create a classification prompt in TabLLM's format.\n",
    "    Based on their YAML templates (e.g., templates_heart.yaml)\n",
    "    \n",
    "    Args:\n",
    "        text: Serialized row text\n",
    "        question: Classification question\n",
    "        choices: List of answer options\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted prompt\n",
    "    \"\"\"\n",
    "    prompt = f\"{text}\\n\\nQuestion: {question}\\nAnswer choices: {', '.join(choices)}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Test the serialization function\n",
    "test_row = pd.Series({\n",
    "    'age': '30-35',\n",
    "    'feeling_sad_or_tearful': 'Yes',\n",
    "    'irritable_towards_baby_partner': 'No',\n",
    "    'feeling_anxious': 'Yes'\n",
    "})\n",
    "\n",
    "print(\"Example serialization:\")\n",
    "print(serialize_row_to_text(test_row))\n",
    "print(\"\\nExample prompt:\")\n",
    "print(create_classification_prompt(\n",
    "    serialize_row_to_text(test_row),\n",
    "    \"Does this person show signs of postpartum depression?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Configuration\n",
    "\n",
    "Define configurations for each dataset including file paths, target columns, and classification questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configurations\n",
    "DATASET_CONFIGS = {\n",
    "    'postpartum_depression': {\n",
    "        'train_file': 'newdata/train_postpartum_depression.csv',\n",
    "        'test_file': 'newdata/test_postpartum_depression.csv',\n",
    "        'target_col': 'feeling anxious',\n",
    "        'exclude_cols': ['timestamp', 'feeling anxious'],\n",
    "        'question': 'Does this person experience feeling anxious (a predictor of postpartum depression)?',\n",
    "        'positive_label': 'Yes',\n",
    "        'negative_label': 'No',\n",
    "        'label_map': {'No': 0, 'Maybe': 1, 'Sometimes': 1, 'Often': 1, 'Yes': 1}  # Binarize\n",
    "    },\n",
    "    'student_depression': {\n",
    "        'train_file': 'newdata/train_student_depression_sample.csv',\n",
    "        'test_file': 'newdata/test_student_depression_sample.csv',\n",
    "        'target_col': 'Depression',\n",
    "        'exclude_cols': ['id', 'Depression'],\n",
    "        'question': 'Does this student have depression?',\n",
    "        'positive_label': 'Yes',\n",
    "        'negative_label': 'No',\n",
    "        'label_map': {0: 0, 1: 1}\n",
    "    },\n",
    "    'ai_tools_usage': {\n",
    "        'train_file': 'newdata/train_StudentsAITools.csv',\n",
    "        'test_file': 'newdata/test_StudentsAITools.csv',\n",
    "        'target_col': 'willing_to_pay_for_access',\n",
    "        'exclude_cols': ['willing_to_pay_for_access'],\n",
    "        'question': 'Is this student willing to pay for AI tool access?',\n",
    "        'positive_label': 'Yes',\n",
    "        'negative_label': 'No',\n",
    "        'label_map': {'No': 0, 'Yes': 1}\n",
    "    },\n",
    "    'hilton_employee': {\n",
    "        'train_file': 'newdata/train_HiltonEmployee.csv',\n",
    "        'test_file': 'newdata/test_HiltonEmployee.csv',\n",
    "        'target_col': 'intenttostayhl',\n",
    "        'exclude_cols': ['inncode', 'intenttostayhl', 'recommendhl'],  # Exclude ID and target-related cols\n",
    "        'question': 'Does this employee intend to stay at Hilton?',\n",
    "        'positive_label': 'Yes',\n",
    "        'negative_label': 'No',\n",
    "        'label_map': {0: 0, 1: 1},\n",
    "        'feature_descriptions': {  # Add meaningful descriptions for key features\n",
    "            'generation': 'employee generation',\n",
    "            'department': 'department',\n",
    "            'fulltimeparttime': 'employment type (full-time or part-time)',\n",
    "            'tenure': 'tenure category',\n",
    "            'managementlevel': 'management level',\n",
    "            'engagement': 'overall engagement score',\n",
    "            'wellbeing': 'wellbeing score',\n",
    "            'workenvironment': 'work environment score',\n",
    "            'learningdevelopment': 'learning and development score',\n",
    "            'worklifebalance': 'work-life balance score',\n",
    "            'rewardsbenefits': 'rewards and benefits score'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Dataset configurations loaded:\")\n",
    "for name, config in DATASET_CONFIGS.items():\n",
    "    print(f\"  - {name}: {config['question']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preprocessing\n",
    "\n",
    "Load datasets and apply text template serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_serialize_dataset(config, dataset_name):\n",
    "    \"\"\"\n",
    "    Load a dataset and apply text serialization.\n",
    "    \n",
    "    Args:\n",
    "        config: Dataset configuration dictionary\n",
    "        dataset_name: Name of the dataset for logging\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_df, test_df) with serialized text and binary labels\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(config['train_file'])\n",
    "    test_df = pd.read_csv(config['test_file'])\n",
    "    \n",
    "    print(f\"Train shape: {train_df.shape}\")\n",
    "    print(f\"Test shape: {test_df.shape}\")\n",
    "    \n",
    "    # Extract and map labels\n",
    "    label_map = config['label_map']\n",
    "    train_df['label'] = train_df[config['target_col']].map(label_map)\n",
    "    test_df['label'] = test_df[config['target_col']].map(label_map)\n",
    "    \n",
    "    # Handle any unmapped labels\n",
    "    if train_df['label'].isna().any() or test_df['label'].isna().any():\n",
    "        print(f\"Warning: Found unmapped labels. Unique values:\")\n",
    "        print(f\"  Train: {train_df[config['target_col']].unique()}\")\n",
    "        print(f\"  Test: {test_df[config['target_col']].unique()}\")\n",
    "        # Drop rows with unmapped labels\n",
    "        train_df = train_df.dropna(subset=['label'])\n",
    "        test_df = test_df.dropna(subset=['label'])\n",
    "    \n",
    "    train_df['label'] = train_df['label'].astype(int)\n",
    "    test_df['label'] = test_df['label'].astype(int)\n",
    "    \n",
    "    # Print label distribution\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(f\"  Train: {train_df['label'].value_counts().to_dict()}\")\n",
    "    print(f\"  Test: {test_df['label'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Serialize rows to text\n",
    "    feature_descriptions = config.get('feature_descriptions', None)\n",
    "    \n",
    "    print(\"\\nSerializing rows to text...\")\n",
    "    train_df['text'] = train_df.apply(\n",
    "        lambda row: serialize_row_to_text(\n",
    "            row, \n",
    "            exclude_cols=config['exclude_cols'],\n",
    "            feature_descriptions=feature_descriptions\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    test_df['text'] = test_df.apply(\n",
    "        lambda row: serialize_row_to_text(\n",
    "            row, \n",
    "            exclude_cols=config['exclude_cols'],\n",
    "            feature_descriptions=feature_descriptions\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create prompts\n",
    "    question = config['question']\n",
    "    choices = [config['negative_label'], config['positive_label']]\n",
    "    \n",
    "    train_df['prompt'] = train_df['text'].apply(\n",
    "        lambda x: create_classification_prompt(x, question, choices)\n",
    "    )\n",
    "    test_df['prompt'] = test_df['text'].apply(\n",
    "        lambda x: create_classification_prompt(x, question, choices)\n",
    "    )\n",
    "    \n",
    "    # Create target text (verbalizer)\n",
    "    train_df['target'] = train_df['label'].map({0: config['negative_label'], 1: config['positive_label']})\n",
    "    test_df['target'] = test_df['label'].map({0: config['negative_label'], 1: config['positive_label']})\n",
    "    \n",
    "    # Show example\n",
    "    print(f\"\\nExample serialized row:\")\n",
    "    print(f\"Text: {train_df['text'].iloc[0][:200]}...\")\n",
    "    print(f\"\\nFull prompt: {train_df['prompt'].iloc[0][:300]}...\")\n",
    "    print(f\"Target: {train_df['target'].iloc[0]}\")\n",
    "    \n",
    "    # Check token lengths (approximate)\n",
    "    avg_tokens_train = train_df['prompt'].str.split().str.len().mean()\n",
    "    max_tokens_train = train_df['prompt'].str.split().str.len().max()\n",
    "    print(f\"\\nToken statistics (approximate):\")\n",
    "    print(f\"  Average tokens: {avg_tokens_train:.0f}\")\n",
    "    print(f\"  Max tokens: {max_tokens_train:.0f}\")\n",
    "    \n",
    "    if max_tokens_train > 800:  # Leave room for actual tokenization overhead\n",
    "        print(f\"  Warning: Some prompts may exceed T0's 1024 token limit!\")\n",
    "    \n",
    "    return train_df[['prompt', 'target', 'label']], test_df[['prompt', 'target', 'label']]\n",
    "\n",
    "\n",
    "# Load all datasets\n",
    "datasets = {}\n",
    "for name, config in DATASET_CONFIGS.items():\n",
    "    train_df, test_df = load_and_serialize_dataset(config, name)\n",
    "    datasets[name] = {\n",
    "        'train': train_df,\n",
    "        'test': test_df,\n",
    "        'config': config\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Setup: T0 with LoRA\n",
    "\n",
    "Following TabLLM's approach, we use T0 (T0-3B) with parameter-efficient fine-tuning.\n",
    "They used IA3 adapters, but we'll use LoRA which is more widely available and similarly efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"bigscience/T0_3B\"  # T0-3B model (3 billion parameters)\n",
    "# Alternative: \"bigscience/T0pp\" (11B, requires more memory)\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# T0 models don't have a pad token, so we set it to eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
    "print(f\"Max model length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_model(model_name=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Create T0 model with LoRA adapters for parameter-efficient fine-tuning.\n",
    "    \n",
    "    Based on TabLLM's IA3 configuration:\n",
    "    - They used learning rate 0.003, batch size 4\n",
    "    - IA3 adds ~0.1% trainable parameters\n",
    "    - LoRA is similar, we configure it for comparable efficiency\n",
    "    \"\"\"\n",
    "    # Load base model in 8-bit for memory efficiency (important for Colab)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,  # Quantization for memory efficiency\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # LoRA configuration\n",
    "    # Following TabLLM's parameter-efficient approach\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=8,  # LoRA rank - lower = more efficient\n",
    "        lora_alpha=32,  # LoRA scaling factor\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q\", \"v\"],  # Apply LoRA to query and value projections\n",
    "        inference_mode=False\n",
    "    )\n",
    "    \n",
    "    # Wrap model with LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# We'll create the model fresh for each dataset to avoid cross-contamination\n",
    "print(\"Model setup function ready.\")\n",
    "print(\"Models will be created per-dataset during training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_training(df, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Convert DataFrame to HuggingFace Dataset and tokenize.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'prompt', 'target', 'label' columns\n",
    "        tokenizer: Tokenizer instance\n",
    "        max_length: Maximum sequence length (T0 supports up to 1024)\n",
    "    \n",
    "    Returns:\n",
    "        Dataset: Tokenized dataset ready for training\n",
    "    \"\"\"\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize inputs (prompts)\n",
    "        model_inputs = tokenizer(\n",
    "            examples['prompt'],\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets (labels)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples['target'],\n",
    "                max_length=3,  # \"Yes\" or \"No\" is very short\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors=None\n",
    "            )\n",
    "        \n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "def compute_metrics_comprehensive(predictions, references, positive_label_text, negative_label_text, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute comprehensive evaluation metrics as requested.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions (logits)\n",
    "        references: Ground truth labels\n",
    "        positive_label_text: Text for positive class (e.g., \"Yes\")\n",
    "        negative_label_text: Text for negative class (e.g., \"No\")\n",
    "        tokenizer: Tokenizer to decode predictions\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Decode predictions\n",
    "    pred_ids = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Get token IDs for Yes/No\n",
    "    positive_token_id = tokenizer.encode(positive_label_text, add_special_tokens=False)[0]\n",
    "    negative_token_id = tokenizer.encode(negative_label_text, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Convert predictions to binary labels\n",
    "    # Extract first non-pad token from predictions\n",
    "    pred_labels = []\n",
    "    for pred_seq in pred_ids:\n",
    "        first_token = pred_seq[0]  # First token should be Yes/No\n",
    "        if first_token == positive_token_id:\n",
    "            pred_labels.append(1)\n",
    "        elif first_token == negative_token_id:\n",
    "            pred_labels.append(0)\n",
    "        else:\n",
    "            # Default to negative if unclear\n",
    "            pred_labels.append(0)\n",
    "    \n",
    "    pred_labels = np.array(pred_labels)\n",
    "    \n",
    "    # Convert references to binary labels\n",
    "    true_labels = []\n",
    "    for ref_seq in references:\n",
    "        # Filter out padding tokens (-100)\n",
    "        ref_seq_filtered = [t for t in ref_seq if t != -100]\n",
    "        if len(ref_seq_filtered) > 0:\n",
    "            first_token = ref_seq_filtered[0]\n",
    "            if first_token == positive_token_id:\n",
    "                true_labels.append(1)\n",
    "            else:\n",
    "                true_labels.append(0)\n",
    "        else:\n",
    "            true_labels.append(0)\n",
    "    \n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Get probabilities for positive class (for AUC and log loss)\n",
    "    # Take softmax over the first token position for Yes/No tokens\n",
    "    yes_no_logits = predictions[:, 0, [negative_token_id, positive_token_id]]\n",
    "    probs = torch.softmax(torch.tensor(yes_no_logits), dim=-1).numpy()\n",
    "    pred_probs = probs[:, 1]  # Probability of positive class\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    \n",
    "    # Precision, Recall, F1 (overall and per-class)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        true_labels, pred_labels, average='macro', zero_division=0\n",
    "    )\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        true_labels, pred_labels, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # AUC and Log Loss\n",
    "    try:\n",
    "        auc = roc_auc_score(true_labels, pred_probs)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    try:\n",
    "        logloss = log_loss(true_labels, pred_probs)\n",
    "    except:\n",
    "        logloss = float('inf')\n",
    "    \n",
    "    # Confusion matrix for additional insights\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'log_loss': logloss,\n",
    "        'precision_overall': precision_macro,\n",
    "        'recall_overall': recall_macro,\n",
    "        'f1_overall': f1_macro,\n",
    "        'precision_negative': precision_per_class[0] if len(precision_per_class) > 0 else 0.0,\n",
    "        'precision_positive': precision_per_class[1] if len(precision_per_class) > 1 else 0.0,\n",
    "        'recall_negative': recall_per_class[0] if len(recall_per_class) > 0 else 0.0,\n",
    "        'recall_positive': recall_per_class[1] if len(recall_per_class) > 1 else 0.0,\n",
    "        'f1_negative': f1_per_class[0] if len(f1_per_class) > 0 else 0.0,\n",
    "        'f1_positive': f1_per_class[1] if len(f1_per_class) > 1 else 0.0,\n",
    "    }\n",
    "    \n",
    "    return metrics, cm\n",
    "\n",
    "\n",
    "print(\"Training and evaluation functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine-tuning Loop\n",
    "\n",
    "Train T0 with LoRA on each dataset and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_dataset(dataset_name, data, output_dir='./results'):\n",
    "    \"\"\"\n",
    "    Train and evaluate T0 model on a single dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        data: Dictionary with 'train', 'test', 'config' keys\n",
    "        output_dir: Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(f\"TRAINING: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create output directory\n",
    "    dataset_output_dir = f\"{output_dir}/{dataset_name}\"\n",
    "    os.makedirs(dataset_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get data\n",
    "    train_df = data['train']\n",
    "    test_df = data['test']\n",
    "    config = data['config']\n",
    "    \n",
    "    # Create fresh model for this dataset\n",
    "    print(\"Loading model...\")\n",
    "    model = create_lora_model(MODEL_NAME)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    print(\"\\nPreparing datasets...\")\n",
    "    train_dataset = prepare_dataset_for_training(train_df, tokenizer, max_length=512)\n",
    "    test_dataset = prepare_dataset_for_training(test_df, tokenizer, max_length=512)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    # Based on TabLLM: lr=0.003, batch_size=4, gradient_accumulation=1\n",
    "    # Adapted for full dataset training (not just k-shot)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=dataset_output_dir,\n",
    "        num_train_epochs=3,  # Adjust based on dataset size\n",
    "        per_device_train_batch_size=4,  # Same as TabLLM\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=3e-3,  # Same as TabLLM (0.003)\n",
    "        warmup_steps=100,\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        fp16=True,  # Mixed precision for speed\n",
    "        gradient_accumulation_steps=1,\n",
    "        report_to=\"none\",  # Disable wandb/tensorboard\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Evaluation samples: {len(test_dataset)}\")\n",
    "    print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "    print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"Learning rate: {training_args.learning_rate}\\n\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\n\\nEvaluating on test set...\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions_output = trainer.predict(test_dataset)\n",
    "    predictions = predictions_output.predictions\n",
    "    references = predictions_output.label_ids\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics, cm = compute_metrics_comprehensive(\n",
    "        predictions,\n",
    "        references,\n",
    "        positive_label_text=config['positive_label'],\n",
    "        negative_label_text=config['negative_label'],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"RESULTS: {dataset_name.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nLog Loss:          {metrics['log_loss']:.4f}\")\n",
    "    print(f\"AUC:               {metrics['auc']:.4f}\")\n",
    "    print(f\"Accuracy:          {metrics['accuracy']:.4f}\")\n",
    "    print(f\"\\nPrecision (Overall): {metrics['precision_overall']:.4f}\")\n",
    "    print(f\"  - Negative:        {metrics['precision_negative']:.4f}\")\n",
    "    print(f\"  - Positive:        {metrics['precision_positive']:.4f}\")\n",
    "    print(f\"\\nRecall (Overall):    {metrics['recall_overall']:.4f}\")\n",
    "    print(f\"  - Negative:        {metrics['recall_negative']:.4f}\")\n",
    "    print(f\"  - Positive:        {metrics['recall_positive']:.4f}\")\n",
    "    print(f\"\\nF1 Score (Overall):  {metrics['f1_overall']:.4f}\")\n",
    "    print(f\"  - Negative:        {metrics['f1_negative']:.4f}\")\n",
    "    print(f\"  - Positive:        {metrics['f1_positive']:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Save results\n",
    "    results_dict = {\n",
    "        'dataset': dataset_name,\n",
    "        **metrics\n",
    "    }\n",
    "    results_df = pd.DataFrame([results_dict])\n",
    "    results_df.to_csv(f\"{dataset_output_dir}/metrics.csv\", index=False)\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    cm_df = pd.DataFrame(cm, \n",
    "                         columns=['Predicted Negative', 'Predicted Positive'],\n",
    "                         index=['True Negative', 'True Positive'])\n",
    "    cm_df.to_csv(f\"{dataset_output_dir}/confusion_matrix.csv\")\n",
    "    \n",
    "    # Clean up to save memory\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "\n",
    "print(\"Training function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Training on All Datasets\n",
    "\n",
    "**Note:** This will take several hours on Colab's free GPU. \n",
    "You can comment out datasets you don't want to train immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all datasets\n",
    "all_results = []\n",
    "\n",
    "for dataset_name, data in datasets.items():\n",
    "    try:\n",
    "        results = train_and_evaluate_dataset(dataset_name, data)\n",
    "        all_results.append(results)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError training {dataset_name}: {str(e)}\")\n",
    "        print(f\"Skipping to next dataset...\\n\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\\nAll training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Consolidated Results\n",
    "\n",
    "View all results in a single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create consolidated results table\n",
    "if len(all_results) > 0:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Reorder columns for readability\n",
    "    column_order = [\n",
    "        'dataset',\n",
    "        'log_loss',\n",
    "        'auc',\n",
    "        'accuracy',\n",
    "        'recall_overall',\n",
    "        'recall_positive',\n",
    "        'recall_negative',\n",
    "        'precision_overall',\n",
    "        'precision_positive',\n",
    "        'precision_negative',\n",
    "        'f1_overall',\n",
    "        'f1_positive',\n",
    "        'f1_negative'\n",
    "    ]\n",
    "    \n",
    "    results_df = results_df[column_order]\n",
    "    \n",
    "    # Save consolidated results\n",
    "    results_df.to_csv('./results/consolidated_results.csv', index=False)\n",
    "    \n",
    "    # Display\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"CONSOLIDATED RESULTS - ALL DATASETS\")\n",
    "    print(\"=\"*100)\n",
    "    print(results_df.to_string(index=False))\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\\nSummary Statistics Across Datasets:\")\n",
    "    print(\"-\" * 60)\n",
    "    summary = results_df.drop('dataset', axis=1).describe().loc[['mean', 'std', 'min', 'max']]\n",
    "    print(summary.to_string())\n",
    "    \n",
    "else:\n",
    "    print(\"No results to display. Training may have failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Additional Analysis (Optional)\n",
    "\n",
    "Compare with baselines, analyze errors, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('TabLLM Results Across Datasets', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics_to_plot = [\n",
    "        ('accuracy', 'Accuracy'),\n",
    "        ('auc', 'AUC'),\n",
    "        ('f1_overall', 'F1 Score (Overall)'),\n",
    "        ('log_loss', 'Log Loss')\n",
    "    ]\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        values = results_df[metric].values\n",
    "        datasets_names = results_df['dataset'].values\n",
    "        \n",
    "        bars = ax.bar(range(len(values)), values, color='steelblue', alpha=0.7)\n",
    "        ax.set_xticks(range(len(values)))\n",
    "        ax.set_xticklabels(datasets_names, rotation=45, ha='right')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(title)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./results/metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPlot saved to ./results/metrics_comparison.png\")\n",
    "else:\n",
    "    print(\"No results to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully replicates the TabLLM approach:\n",
    "\n",
    "1. **Text Serialization**: Converted tabular rows to natural language using \"The [column] is [value]\" format\n",
    "2. **Prompting**: Created task-specific classification prompts following TabLLM's template structure\n",
    "3. **Fine-tuning**: Used T0-3B with LoRA adapters (similar efficiency to their IA3 approach)\n",
    "4. **Training**: Fine-tuned on full training data with TabLLM's hyperparameters (lr=0.003, batch_size=4)\n",
    "5. **Evaluation**: Computed comprehensive metrics across all datasets\n",
    "\n",
    "**Key Adaptations for Colab:**\n",
    "- Used 8-bit quantization for memory efficiency\n",
    "- LoRA instead of IA3 (more widely available, similar efficiency)\n",
    "- Optimized batch sizes and gradient accumulation for T4 GPU\n",
    "\n",
    "**Results:**\n",
    "See the consolidated table above and individual dataset results in `./results/` folder."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}